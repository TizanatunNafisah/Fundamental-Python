{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jumadi-cloud/Fundamental-Python/blob/main/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6FrC5uvbBLv"
      },
      "source": [
        "# 1. Import Library yang kita butuhkan\n",
        "\n",
        "- Pada perintah di bawah kami mengimport semua kebutuhan library yang kami butuhkan,\n",
        "- Jika temen temen tidak mau import semua library yang di butuhkan dalam step 1 temen temen cukup import library json saja untuk menghubungkan dataset yang sudah kita buat.\n",
        "\n",
        "### Perlu di note sebelumnya di virtual environment kami sudah Install and import library yang di butuhkan seperti **tensorflow, keras, keras-models, pickle, nltk**\n",
        "\n",
        "Cara install:\n",
        "\n",
        "- pip install tensorflow\n",
        "- pip install keras\n",
        "- pip install keras-models\n",
        "- pip install pickle\n",
        "- pip install nltk\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxBsx23IbBL3"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n",
        "from keras.models import load_model\n",
        "\n",
        "# create an object of WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# importing the GL Bot corpus file for pre-processing\n",
        "\n",
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open(\"intents.json\").read()\n",
        "intents = json.loads(data_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqEf8pr-bBL5"
      },
      "source": [
        "# 2. Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3wA2nrNbBL5"
      },
      "outputs": [],
      "source": [
        "# preprocessing the json data\n",
        "# tokenization\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('wordnet')\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # add to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdAtHd7ibBL6"
      },
      "source": [
        "### Tokenisasi\n",
        "\n",
        "- Pada proses tokenisasi pada dasarnya adalah pemisahan kalimat, paragraf, \n",
        "atau seluruh dokumen teks menjadi unit yang lebih kecil, proses itu yang disebut token\n",
        "\n",
        "- Pada proses ini juga akan save documen tersebut menjadi file label.pkl dan texts.pkl (proses labeling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPAFwOhUbBL6"
      },
      "outputs": [],
      "source": [
        "\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "\n",
        "# creating a pickle file to store the Python objects which we will use while predicting\n",
        "pickle.dump(words,open('texts.pkl','wb')) \n",
        "pickle.dump(classes,open('label.pkl','wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekWBghDbBL7"
      },
      "source": [
        "# 3. Creating Training Data\n",
        "\n",
        "- Pada dasarnya, bag of words adalah representasi sederhana dari setiap teks dalam sebuah kalimat sebagai bag of words-nya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66vXO1oFbBL7"
      },
      "outputs": [],
      "source": [
        "# create our training data\n",
        "training = []\n",
        "\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "   \n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    \n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle features and converting it into numpy arrays\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "\n",
        "print(\"Training data created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJzQeE9FbBL8"
      },
      "source": [
        "# 5. Creating Modeling \n",
        "\n",
        "- Pada proses ini kami akan membuat model jaringan saraf dan menyimpan model tersebut "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWNEo2ShbBL8"
      },
      "outputs": [],
      "source": [
        "# Create NN model to predict the responses\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
        "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "#fitting and saving the model \n",
        "hist = model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)\n",
        "model.save('model.h5', hist) # we will pickle this model to use in the future\n",
        "print(\"\\n\")\n",
        "print(\"*\"*50)\n",
        "print(\"\\nModel Created Successfully!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.13 ('prakerja')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "1e0f4fe6ccee135700700c04fb47eca4b2c8b959d86ecafd53b43ebd6f201da1"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}